{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08aafab7",
   "metadata": {},
   "source": [
    "## Run the inference with the PyTorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea03f71",
   "metadata": {},
   "source": [
    "### 1. Superpoint Open-Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightglue_dynamo.models.superpoint_pytorch as superpoint_pytorch\n",
    "\n",
    "detection_thresh = 0.005\n",
    "nms_radius = 5\n",
    "max_keypoints = 1024\n",
    "\n",
    "sp_th = superpoint_pytorch.SuperPointOpen(detection_threshold=detection_thresh, nms_radius=nms_radius, max_num_keypoints=max_keypoints).eval()\n",
    "print('Config:', sp_th.conf)\n",
    "\n",
    "# Load state dict (map_location=\"cpu\" if no GPU)\n",
    "ckpt = torch.load(\"./weights/superpoint_v6_from_tf.pth\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Some checkpoints may be wrapped in {\"state_dict\": ...}\n",
    "if \"state_dict\" in ckpt:\n",
    "    ckpt = ckpt[\"state_dict\"]\n",
    "\n",
    "# Load weights\n",
    "sp_th.load_state_dict(ckpt, strict=True)\n",
    "sp_th.eval().cuda()\n",
    "# sp_th = torch.compile(sp_th, mode=\"reduce-overhead\")  # \"max-autotune\" , \"reduce-overhead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from lightglue_dynamo.preprocessors import SuperPointOpenPreprocessor\n",
    "from lightglue_dynamo import viz\n",
    "\n",
    "# Load and Normalize Images Individually\n",
    "image_path1 = './assets/sacre_coeur1.jpg' # sacre_coeur1.jpg, debug1.png\n",
    "image_path2 = './assets/sacre_coeur2.jpg' # sacre_coeur2.jpg, debug2.png\n",
    "\n",
    "h, w = 360, 640 \n",
    "\n",
    "# Load color images and resize them\n",
    "raw_img1 = cv2.resize(cv2.imread(image_path1), (w, h))\n",
    "raw_img2 = cv2.resize(cv2.imread(image_path2), (w, h))\n",
    "image_batch_bgr = np.stack([raw_img1, raw_img2], axis=0)\n",
    "preprocessed_batch = SuperPointOpenPreprocessor.preprocess(image_batch_bgr)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor = torch.from_numpy(preprocessed_batch).to(device)\n",
    "\n",
    "print(f\"Final tensor shape: {tensor.shape}\") #torch.Size([2, 1, H_padded, W_padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with torch.no_grad():\n",
    "    t0 = time.time()\n",
    "    pred = sp_th({\"image\": tensor})\n",
    "    t1 = time.time()\n",
    "\n",
    "print(f\"Inference Time in ms: {(t1-t0)*1000}\")\n",
    "\n",
    "kpts = pred['keypoints'].cpu().numpy()\n",
    "scores = pred['keypoint_scores'].cpu().numpy()\n",
    "descriptors = pred['descriptors'].cpu().numpy()\n",
    "num_kpts = pred['num_keypoints'].cpu().numpy()\n",
    "\n",
    "print(f\"Keypoints: {kpts.shape}\")\n",
    "print(f\"Keypoint scores: {scores.shape}\")\n",
    "print(f\"Descriptors: {descriptors.shape}\")\n",
    "print(f\"Number of keypoints: {num_kpts.shape}\")\n",
    "\n",
    "viz.plot_extractor_only(image_batch_bgr, image_batch_bgr.shape[0], kpts, num_kpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d431ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model_path = './weights/superpoint_open_b2_h360_w640_kp256_simplify.onnx'\n",
    "session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "print(f\"Input Name: {input_name}\")\n",
    "print(f\"Output Names: {output_names}\")\n",
    "\n",
    "# Run Inference\n",
    "onnx_inputs = {input_name: preprocessed_batch}\n",
    "print(\"Preprocessed batch:\", preprocessed_batch.shape, preprocessed_batch.dtype,\n",
    "      preprocessed_batch.min(), preprocessed_batch.max())\n",
    "\n",
    "outputs_onnx = session.run(output_names, onnx_inputs)\n",
    "\n",
    "# Unpack the list of outputs\n",
    "kpts_onnx, scores_onnx, desc_onnx, num_kpts_onnx = outputs_onnx\n",
    "\n",
    "print(\"\\n--- ONNX Runtime Output ---\")\n",
    "print(f\"Keypoints shape: {kpts_onnx.shape}\")\n",
    "print(f\"Scores shape: {scores_onnx.shape}\")\n",
    "print(f\"Descriptors shape: {desc_onnx.shape}\")\n",
    "print(f\"Num Keypoints: {num_kpts_onnx}\")\n",
    "print(\"Num keypoints (ONNX):\", num_kpts_onnx)\n",
    "print(\"Num keypoints (sum):\", sum(num_kpts_onnx) if num_kpts_onnx.ndim > 0 else num_kpts_onnx)\n",
    "\n",
    "viz.plot_extractor_only(image_batch_bgr, image_batch_bgr.shape[0], kpts_onnx, num_kpts_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "def validate_export():\n",
    "    \"\"\"Run after export to validate it matches PyTorch\"\"\"\n",
    "    \n",
    "    from lightglue_dynamo.models.superpoint_pytorch import SuperPointOpen\n",
    "    \n",
    "    detection_thresh = 0.005\n",
    "    nms_radius = 5\n",
    "    max_keypoints = 256\n",
    "    \n",
    "    extractor = SuperPointOpen(\n",
    "        detection_threshold=detection_thresh, \n",
    "        nms_radius=nms_radius, \n",
    "        max_num_keypoints=max_keypoints\n",
    "    )\n",
    "    \n",
    "    # Load same weights as export\n",
    "    ckpt = torch.load(\"./weights/superpoint_v6_from_tf.pth\", map_location=\"cpu\")\n",
    "    if \"state_dict\" in ckpt:\n",
    "        ckpt = ckpt[\"state_dict\"]\n",
    "    \n",
    "    extractor.load_state_dict(ckpt, strict=True)\n",
    "    extractor.eval()\n",
    "    \n",
    "    # Same random seed as export\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create test input\n",
    "    test_input = torch.randn(2, 1, 360, 640, dtype=torch.float32)\n",
    "    \n",
    "    # PyTorch inference\n",
    "    with torch.no_grad():\n",
    "        torch_output = extractor({\"image\": test_input})\n",
    "    \n",
    "    torch_kpts = torch_output[\"keypoints\"].numpy()\n",
    "    torch_scores = torch_output[\"keypoint_scores\"].numpy()\n",
    "    torch_desc = torch_output[\"descriptors\"].numpy()\n",
    "    torch_num = torch_output[\"num_keypoints\"].numpy()\n",
    "    \n",
    "    print(\"=== PyTorch (reference) ===\")\n",
    "    print(f\"Keypoints: {torch_kpts.shape}, range: [{torch_kpts.min():.3f}, {torch_kpts.max():.3f}]\")\n",
    "    print(f\"Scores: {torch_scores.shape}, range: [{torch_scores.min():.3f}, {torch_scores.max():.3f}]\")\n",
    "    print(f\"Descriptors: {torch_desc.shape}, range: [{torch_desc.min():.3f}, {torch_desc.max():.3f}]\")\n",
    "    print(f\"Num keypoints: {torch_num}\")\n",
    "    \n",
    "    # ONNX inference\n",
    "    onnx_path = './weights/superpoint_open_b2_h360_w640_kp256_simplify.onnx'\n",
    "    session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    onnx_output = session.run(None, {\"images\": test_input.numpy()})\n",
    "    onnx_kpts, onnx_scores, onnx_desc, onnx_num = onnx_output\n",
    "    \n",
    "    print(\"\\n=== ONNX ===\")\n",
    "    print(f\"Keypoints: {onnx_kpts.shape}, range: [{onnx_kpts.min():.3f}, {onnx_kpts.max():.3f}]\")\n",
    "    print(f\"Scores: {onnx_scores.shape}, range: [{onnx_scores.min():.3f}, {onnx_scores.max():.3f}]\")\n",
    "    print(f\"Descriptors: {onnx_desc.shape}, range: [{onnx_desc.min():.3f}, {onnx_desc.max():.3f}]\")\n",
    "    print(f\"Num keypoints: {onnx_num}\")\n",
    "    \n",
    "    # Compare\n",
    "    print(\"\\n=== Differences ===\")\n",
    "    kpt_diff = np.max(np.abs(torch_kpts - onnx_kpts))\n",
    "    score_diff = np.max(np.abs(torch_scores - onnx_scores))\n",
    "    desc_diff = np.max(np.abs(torch_desc - onnx_desc))\n",
    "    num_diff = np.max(np.abs(torch_num - onnx_num))\n",
    "    \n",
    "    print(f\"Max keypoint diff: {kpt_diff:.6f}\")\n",
    "    print(f\"Max score diff: {score_diff:.6f}\")\n",
    "    print(f\"Max descriptor diff: {desc_diff:.6f}\")\n",
    "    print(f\"Max num_keypoints diff: {num_diff}\")\n",
    "\n",
    "validate_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673324e7",
   "metadata": {},
   "source": [
    "### 2. ALIKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "try:\n",
    "    onnx_model = onnx.load(\"./weights/aliked_n16_b2_h384_w640_kp256_simplify.onnx\")\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model is syntactically valid.\")\n",
    "except Exception as e:\n",
    "    print(f\"ONNX model is invalid: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightglue_dynamo.models import aliked\n",
    "\n",
    "top_k = 1024\n",
    "scores_th = 0.2\n",
    "n_limit = 2048\n",
    "\n",
    "aliked_model = aliked.ALIKED(model_name=\"aliked-n16\",\n",
    "                          device=\"cuda\",\n",
    "                          top_k=top_k,\n",
    "                          scores_th=scores_th,\n",
    "                          n_limit=n_limit,\n",
    "                          pretrained_path=\"./weights/aliked-n16.pth\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe369f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from lightglue_dynamo.preprocessors import ALIKEDPreprocessor\n",
    "from lightglue_dynamo import viz\n",
    "\n",
    "# Load and Normalize Images Individually\n",
    "image_path1 = './assets/sacre_coeur1.jpg' # sacre_coeur1.jpg, debug1.png\n",
    "image_path2 = './assets/sacre_coeur2.jpg' # sacre_coeur2.jpg, debug2.png\n",
    "\n",
    "h, w = 384, 640 \n",
    "\n",
    "# Load color images and resize them\n",
    "raw_img1 = cv2.resize(cv2.imread(image_path1), (w, h))\n",
    "raw_img2 = cv2.resize(cv2.imread(image_path2), (w, h))\n",
    "image_batch_bgr = np.stack([raw_img1, raw_img2], axis=0)\n",
    "preprocessed_batch = ALIKEDPreprocessor.preprocess(image_batch_bgr)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor = torch.from_numpy(preprocessed_batch).float().to(device)\n",
    "\n",
    "print(f\"Final tensor shape: {tensor.shape}\") #torch.Size([2, 1, H_padded, W_padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with torch.no_grad():\n",
    "    # Model forward pass\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions = aliked_model(tensor)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "print(f\"Inference took: {(end_time - start_time) * 1000:.2f} ms\")\n",
    "\n",
    "kpts0 = predictions['keypoints'][0].cpu().numpy()\n",
    "kpts1 = predictions['keypoints'][1].cpu().numpy()\n",
    "scores0 = predictions['scores'][0].cpu().numpy()\n",
    "scores1 = predictions['scores'][1].cpu().numpy()\n",
    "\n",
    "wh = torch.tensor([w - 1, h - 1], device=device, dtype=torch.float)\n",
    "kpts_unnorm_list = [((kpts + 1) / 2 * wh).cpu().numpy() for kpts in predictions['keypoints']]\n",
    "kpts_unnorm_np = np.stack(\n",
    "    [((kpts + 1) / 2 * wh).cpu().numpy() for kpts in predictions['keypoints']],\n",
    "    axis=0\n",
    ")  # shape: [batch, N, 2]\n",
    "\n",
    "scores = predictions['scores']\n",
    "num_valid_kpts = np.array([(s > scores_th).sum().item() for s in scores])\n",
    "print(f\"Number of valid keypoints found: {num_valid_kpts}\")\n",
    "\n",
    "viz.plot_extractor_only(\n",
    "    images=image_batch_bgr,\n",
    "    batch_size=len(image_batch_bgr),\n",
    "    kpts=kpts_unnorm_np,\n",
    "    num_kpts=num_valid_kpts,\n",
    "    extractor_name=\"ALIKED\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102de269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model_path = './weights/aliked_n16_b2_h384_w640_kp256_simplify.onnx'\n",
    "session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "print(f\"Input Name: {input_name}\")\n",
    "print(f\"Output Names: {output_names}\")\n",
    "\n",
    "# Run Inference\n",
    "onnx_inputs = {input_name: preprocessed_batch}\n",
    "print(\"Preprocessed batch:\", preprocessed_batch.shape, preprocessed_batch.dtype,\n",
    "      preprocessed_batch.min(), preprocessed_batch.max())\n",
    "\n",
    "outputs_onnx = session.run(output_names, onnx_inputs)\n",
    "\n",
    "# Unpack the list of outputs\n",
    "kpts_onnx, scores_onnx, desc_onnx, num_kpts_onnx = outputs_onnx\n",
    "\n",
    "print(\"\\n--- ONNX Runtime Output ---\")\n",
    "print(f\"Keypoints shape: {kpts_onnx.shape}\")\n",
    "print(f\"Scores shape: {scores_onnx.shape}\")\n",
    "print(f\"Descriptors shape: {desc_onnx.shape}\")\n",
    "print(f\"Num Keypoints: {num_kpts_onnx}\")\n",
    "print(\"Num keypoints (ONNX):\", num_kpts_onnx)\n",
    "print(\"Num keypoints (sum):\", sum(num_kpts_onnx) if num_kpts_onnx.ndim > 0 else num_kpts_onnx)\n",
    "\n",
    "viz.plot_extractor_only(image_batch_bgr, image_batch_bgr.shape[0], kpts_onnx, num_kpts_onnx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightglue-onnx--g632ONZ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
