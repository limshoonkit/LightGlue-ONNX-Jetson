{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08aafab7",
   "metadata": {},
   "source": [
    "## Run the inference with the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightglue_dynamo.models.superpoint_pytorch as superpoint_pytorch\n",
    "\n",
    "detection_thresh = 0.005\n",
    "nms_radius = 5\n",
    "max_keypoints = 256\n",
    "\n",
    "sp_th = superpoint_pytorch.SuperPointOpen(detection_threshold=detection_thresh, nms_radius=nms_radius, max_num_keypoints=max_keypoints).eval()\n",
    "print('Config:', sp_th.conf)\n",
    "\n",
    "# Load state dict (map_location=\"cpu\" if no GPU)\n",
    "ckpt = torch.load(\"/home/nvidia/third_party/LightGlue-ONNX-Jetson/weights/superpoint_v6_from_tf.pth\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Some checkpoints may be wrapped in {\"state_dict\": ...}\n",
    "if \"state_dict\" in ckpt:\n",
    "    ckpt = ckpt[\"state_dict\"]\n",
    "\n",
    "# Load weights\n",
    "sp_th.load_state_dict(ckpt, strict=True)\n",
    "sp_th.eval().cuda()\n",
    "# sp_th = torch.compile(sp_th, mode=\"reduce-overhead\")  # \"max-autotune\" , \"reduce-overhead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load and Normalize Images Individually ---\n",
    "image_path1 = '/home/nvidia/third_party/LightGlue-ONNX-Jetson/assets/debug1.png'\n",
    "image_path2 = '/home/nvidia/third_party/LightGlue-ONNX-Jetson/assets/debug2.png'\n",
    "\n",
    "# Load as grayscale float32 and normalize\n",
    "img1 = cv2.imread(image_path1, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
    "img2 = cv2.imread(image_path2, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
    "\n",
    "# --- 2. Pad each image individually (simpler) ---\n",
    "# SuperPoint stride is 8\n",
    "stride = 8 \n",
    "H, W = img1.shape\n",
    "pad_h = (stride - H % stride) % stride\n",
    "pad_w = (stride - W % stride) % stride\n",
    "\n",
    "# np.pad takes a tuple of tuples for padding ((top, bottom), (left, right))\n",
    "img1_padded = np.pad(img1, ((0, pad_h), (0, pad_w)), mode='constant')\n",
    "img2_padded = np.pad(img2, ((0, pad_h), (0, pad_w)), mode='constant')\n",
    "\n",
    "# --- 3. Stack to create a batch and add channel dimension ---\n",
    "# Stack along axis=0 to create the batch. Shape: (2, H_padded, W_padded)\n",
    "image_batch = np.stack([img1_padded, img2_padded], axis=0)\n",
    "\n",
    "# Add the channel dimension. Shape: (2, 1, H_padded, W_padded)\n",
    "image_batch_unsqueezed = image_batch[:, None, :, :]\n",
    "\n",
    "# --- 4. Convert to tensor and run inference ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor = torch.from_numpy(image_batch_unsqueezed).to(device)\n",
    "\n",
    "print(f\"Final tensor shape: {tensor.shape}\") #torch.Size([2, 1, H_padded, W_padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = sp_th({\"image\": tensor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kpts = pred['keypoints'].cpu().numpy()\n",
    "scores = pred['keypoint_scores'].cpu().numpy()\n",
    "descriptors = pred['descriptors'].cpu().numpy()\n",
    "num_kpts = pred['num_keypoints'].cpu().numpy()\n",
    "\n",
    "print(f\"Keypoints: {kpts.shape}\")\n",
    "print(f\"Keypoint scores: {scores.shape}\")\n",
    "print(f\"Descriptors: {descriptors.shape}\")\n",
    "print(f\"Number of keypoints: {num_kpts.shape}\")\n",
    "\n",
    "# Reload original images in color for better visualization\n",
    "img1_color = cv2.imread(image_path1)\n",
    "img2_color = cv2.imread(image_path2)\n",
    "images = [img1_color, img2_color]\n",
    "batch_size = tensor.shape[0]\n",
    "\n",
    "output_images = []\n",
    "for i in range(batch_size):\n",
    "    # Get the valid keypoints for this image using num_keypoints\n",
    "    num = num_kpts[i]\n",
    "    kpts_i = kpts[i, :num, :]\n",
    "    \n",
    "    # Convert keypoints to OpenCV's format\n",
    "    # cv2.KeyPoint(x, y, size)\n",
    "    cv_kpts = [cv2.KeyPoint(p[0], p[1], 5) for p in kpts_i]\n",
    "    \n",
    "    # Draw keypoints on the image\n",
    "    img_with_kpts = cv2.drawKeypoints(images[i], cv_kpts, None, color=(0, 255, 0))\n",
    "    output_images.append(img_with_kpts)\n",
    "\n",
    "# Combine images side-by-side\n",
    "combined_image = np.hstack(output_images)\n",
    "\n",
    "# Display using matplotlib (more portable than cv2.imshow)\n",
    "plt.figure(figsize=(16, 8))\n",
    "# OpenCV loads as BGR, matplotlib displays as RGB\n",
    "plt.imshow(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('SuperPoint Keypoints')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightglue-onnx-py3.10 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
